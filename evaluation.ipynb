{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502585b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from plot_manager import PlotManager\n",
    "from dataset import build_dataloaders\n",
    "from bilstm import BiLSTMAttentionClassifier\n",
    "# from attention_inspector import AttentionInspector\n",
    "\n",
    "# ai\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "# eof ai\n",
    "\n",
    "# pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "import matplotlib.pyplot as plt\n",
    "# eof pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1912e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMAttentionClassifier(\n",
    "    pc_vocab=9,\n",
    "    acc_vocab=10,\n",
    "    oct_vocab=7,\n",
    "    dur_vocab=33,\n",
    "    meas_vocab=4,\n",
    "    emb_dim=32,\n",
    "    lstm_hidden=128,\n",
    "    num_classes=12,\n",
    "    dropout=0.3\n",
    ")\n",
    "model.to(device)\n",
    "checkpoint = torch.load(\"best_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(\"Loaded best model from epoch:\", checkpoint[\"epoch\"])\n",
    "\n",
    "_, _, _, test_loader = build_dataloaders(\n",
    "    \"vocab.pkl\", \"dataset.pkl\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionInspector:\n",
    "    def __init__(self, device, model, dataloader):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        # makam\n",
    "        self.makam_vocab = {\n",
    "            \"hicaz\": 0, \"nihavent\": 1, \"ussak\": 2, \"rast\": 3,\n",
    "            \"huzzam\": 4, \"segah\": 5, \"huseyni\": 6, \"mahur\": 7,\n",
    "            \"hicazkar\": 8, \"kurdilihicazkar\": 9, \"muhayyer\": 10, \"saba\": 11\n",
    "        }\n",
    "        self.makam_vocab_inv = {v: k for k, v in self.makam_vocab.items()}\n",
    "        # PC\n",
    "        self.pc_vocab = {\n",
    "            \"PAD\": 0, \"Rest\": 1,\n",
    "            \"G\": 2, \"A\": 3, \"B\": 4, \"C\": 5, \"D\": 6, \"E\": 7, \"F\": 8\n",
    "        }\n",
    "        self.pc_vocab_inv = {v: k for k, v in self.pc_vocab.items()}\n",
    "        # Acc\n",
    "        self.acc_vocab = {\"PAD\": 0, \"\": 1,\n",
    "                          \"#1\": 2, \"#2\": 3, \"#3\": 4, \"#4\": 5, \"#5\": 6, \"#6\": 7, \"#7\": 8, \"#8\": 9}\n",
    "        self.acc_vocab_inv = {v: k for k, v in self.acc_vocab.items()}\n",
    "        # Dur\n",
    "        self.dur_vocab = {\"PAD\": 0,\n",
    "                          0.007812: 1, 0.008929: 2, 0.010417: 3, 0.015625: 4, 0.017857: 5,\n",
    "                          0.020833: 6, 0.025: 7, 0.027778: 8, 0.03125: 9, 0.035714: 10,\n",
    "                          0.041667: 11, 0.05: 12, 0.055556: 13, 0.0625: 14, 0.071429: 15,\n",
    "                          0.083333: 16, 0.1: 17, 0.111111: 18, 0.125: 19, 0.142857: 20,\n",
    "                          0.166667: 21, 0.1875: 22, 0.2: 23, 0.222222: 24, 0.25: 25,\n",
    "                          0.3: 26, 0.333333: 27, 0.375: 28, 0.4: 29, 0.5: 30, 0.666667: 31, 1.0: 32}\n",
    "        self.dur_vocab_inv = {v: k for k, v in self.dur_vocab.items()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_predictions(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        (self.all_labels,\n",
    "         self.all_preds,\n",
    "         self.all_f_names,\n",
    "         self.all_attn_weights,\n",
    "         self.all_true_lengths,\n",
    "         self.all_pcs,\n",
    "         self.all_accs,\n",
    "         self.all_meas,\n",
    "         self.all_durs) = ([], [], [], [], [], [], [], [], [])\n",
    "\n",
    "        for batch in self.dataloader:\n",
    "            f_names = batch[\"f_names\"]\n",
    "            lengths = batch[\"lengths\"]\n",
    "            batch[\"accs\"]\n",
    "            batch[\"meas\"]\n",
    "            batch = {k: v.to(self.device)\n",
    "                     for k, v in batch.items() if k != \"f_names\"}\n",
    "            logits, attn_weights = self.model(batch)\n",
    "            labels = batch[\"labels\"]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            self.all_preds.append(preds.cpu().numpy())\n",
    "            self.all_labels.append(labels.cpu().numpy())\n",
    "            self.all_f_names.append(f_names)\n",
    "            self.all_attn_weights.extend(attn_weights.cpu().numpy())\n",
    "            self.all_true_lengths.append(lengths.cpu().numpy())\n",
    "            self.all_pcs.extend(batch[\"pcs\"].cpu().numpy())\n",
    "            self.all_accs.extend(batch[\"accs\"].cpu().numpy())\n",
    "            self.all_meas.extend(batch[\"meas\"].cpu().numpy())\n",
    "            self.all_durs.extend(batch[\"durs\"].cpu().numpy())\n",
    "\n",
    "        self.all_preds = np.concatenate(self.all_preds)\n",
    "        self.all_labels = np.concatenate(self.all_labels)\n",
    "        self.all_f_names = np.concatenate(self.all_f_names)\n",
    "        self.all_true_lengths = np.concatenate(self.all_true_lengths)\n",
    "\n",
    "        return (\n",
    "            self.all_labels,\n",
    "            self.all_preds,\n",
    "            self.all_f_names,\n",
    "            self.all_attn_weights,\n",
    "            self.all_true_lengths,\n",
    "            self.all_pcs,\n",
    "            self.all_accs,\n",
    "            self.all_meas,\n",
    "            self.all_durs\n",
    "        )\n",
    "\n",
    "    def export_vocabs(self):\n",
    "        return (\n",
    "            self.makam_vocab,\n",
    "            self.makam_vocab_inv,\n",
    "            self.pc_vocab,\n",
    "            self.pc_vocab_inv,\n",
    "            self.acc_vocab,\n",
    "            self.acc_vocab_inv,\n",
    "            self.dur_vocab,\n",
    "            self.dur_vocab_inv\n",
    "        )\n",
    "\n",
    "    def evaluate_classification(self, y_true, y_pred, makam_names):\n",
    "        results = {}\n",
    "\n",
    "        # Overall accuracy\n",
    "        results[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # Precision, Recall, F1 per class\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=None\n",
    "        )\n",
    "\n",
    "        # Macro & weighted\n",
    "        results[\"macro_f1\"] = np.mean(f1)\n",
    "        results[\"weighted_f1\"] = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"weighted\"\n",
    "        )[2]\n",
    "\n",
    "        # Confusion matrix\n",
    "        results[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=makam_names,\n",
    "            digits=4,\n",
    "            output_dict=True\n",
    "        )\n",
    "        results[\"report\"] = report\n",
    "        return results\n",
    "\n",
    "    def get_single_piece_ai(self, piece_idx):\n",
    "        # all_labels, all_preds, all_f_names, all_attn_weights, all_true_lengths\n",
    "        true_len = self.all_true_lengths[piece_idx]\n",
    "\n",
    "        attn_piece = self.all_attn_weights[piece_idx][:true_len]\n",
    "        pcs_piece = self.all_pcs[piece_idx][:true_len]\n",
    "        accs_piece = self.all_accs[piece_idx][:true_len]\n",
    "        meas_piece = self.all_meas[piece_idx][:true_len]\n",
    "        durs_piece = self.all_durs[piece_idx][:true_len]\n",
    "        return (\n",
    "            attn_piece,\n",
    "            pcs_piece,\n",
    "            accs_piece,\n",
    "            meas_piece,\n",
    "            durs_piece\n",
    "        )\n",
    "\n",
    "    def combine_pitch_pc_acc(self, pc_idx, acc_idx):\n",
    "        pc = self.pc_vocab_inv.get(pc_idx, \"UNK\")\n",
    "        acc_name = self.acc_vocab_inv.get(acc_idx, \"natural\")\n",
    "\n",
    "        if pc == \"PAD\":\n",
    "            return \"PAD\"\n",
    "        if pc == \"Rest\":\n",
    "            return \"Rest\"\n",
    "\n",
    "        return f\"{pc}{acc_name}\"\n",
    "\n",
    "    def aggregate_attention_by_pitch(self, attn, pitch_ids):\n",
    "        \"\"\"\n",
    "        attn: (L,)\n",
    "        pcs:  (L,) pitch class ids\n",
    "        \"\"\"\n",
    "        agg = defaultdict(float)\n",
    "        for a, p in zip(attn, pitch_ids):\n",
    "            agg[p] += a\n",
    "        return agg\n",
    "\n",
    "    def aggregate_attention_by_pitch_acc(self, attn, pcs, accs):\n",
    "        \"\"\"\n",
    "        attn: (L,)\n",
    "        pcs:  (L,) pitch class ids\n",
    "        accs: (L,) accidental ids\n",
    "        \"\"\"\n",
    "        agg = defaultdict(float)\n",
    "\n",
    "        for a, pc, acc in zip(attn, pcs, accs):\n",
    "            if pc == 0:  # PAD\n",
    "                continue\n",
    "            agg[(pc, acc)] += float(a)\n",
    "\n",
    "        return agg\n",
    "\n",
    "    def normalize_attention(self, agg_dict):\n",
    "        total = sum(agg_dict.values())\n",
    "        return {k: v / total for k, v in agg_dict.items()}\n",
    "\n",
    "    def aggregate_attention_pitch_acc_duration(self, attn, pcs, accs, durs):\n",
    "        agg = defaultdict(float)\n",
    "\n",
    "        for a, pc, acc, dur in zip(attn, pcs, accs, durs):\n",
    "            if pc == 0:  # PAD\n",
    "                continue\n",
    "            dur_ratio = self.dur_vocab_inv[dur]\n",
    "            agg[(pc, acc)] += float(a * dur_ratio)\n",
    "\n",
    "        return agg\n",
    "\n",
    "    def attention_duration_correlation(self, attn, durs):\n",
    "        attn_arr = []\n",
    "        dur_arr = []\n",
    "        for a, d in zip(attn, durs):\n",
    "            dur_ratio = self.dur_vocab_inv[d]\n",
    "            dur_arr.append(dur_ratio)\n",
    "            attn_arr.append(a)\n",
    "\n",
    "        return spearmanr(attn, durs).correlation\n",
    "\n",
    "    def get_makam_pieces(self, makam_id, correct=True):\n",
    "        \"\"\"\n",
    "        {'hicaz': 0, 'nihavent': 1, 'ussak': 2, 'rast': 3,\n",
    "        'huzzam': 4, 'segah': 5, 'huseyni': 6, 'mahur': 7,\n",
    "        'hicazkar': 8, 'kurdilihicazkar': 9, 'muhayyer': 10, 'saba': 11}\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for i, (y_hat, y) in enumerate(zip(self.all_preds, self.all_labels)):\n",
    "            if y != makam_id:\n",
    "                continue\n",
    "            if correct and y_hat == y:\n",
    "                res.append(i)\n",
    "            if not correct and y_hat != y:\n",
    "                res.append(i)\n",
    "        return np.array(res)\n",
    "\n",
    "    def aggregate_attention_for_makam(self, makam_idx):\n",
    "        num_pieces = len(self.all_pcs)\n",
    "        makam_agg = defaultdict(float)\n",
    "        piece_count = 0\n",
    "\n",
    "        for i in range(num_pieces):\n",
    "            if self.all_labels[i] != makam_idx:\n",
    "                continue\n",
    "\n",
    "            true_len = self.all_true_lengths[i]\n",
    "            attn_piece = self.all_attn_weights[i][:true_len]\n",
    "            pcs_piece = self.all_pcs[i][:true_len]\n",
    "            accs_piece = self.all_accs[i][:true_len]\n",
    "\n",
    "            piece_agg = self.aggregate_attention_by_pitch_acc(\n",
    "                attn_piece, pcs_piece, accs_piece\n",
    "            )\n",
    "\n",
    "            for k, v in piece_agg.items():\n",
    "                makam_agg[k] += v\n",
    "\n",
    "            piece_count += 1\n",
    "\n",
    "        return makam_agg, piece_count\n",
    "\n",
    "    def aggregate_attention_correct_vs_incorrect(self, makam_idx):\n",
    "        agg_correct = defaultdict(float)\n",
    "        agg_incorrect = defaultdict(float)\n",
    "\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "\n",
    "        num_pieces = len(self.all_pcs)\n",
    "        for i in range(num_pieces):\n",
    "            if self.all_labels[i] != makam_idx:\n",
    "                continue\n",
    "\n",
    "            true_len = self.all_true_lengths[i]\n",
    "            attn_piece = self.all_attn_weights[i][:true_len]\n",
    "            pcs_piece = self.all_pcs[i][:true_len]\n",
    "            accs_piece = self.all_accs[i][:true_len]\n",
    "\n",
    "            piece_agg = self.aggregate_attention_by_pitch_acc(\n",
    "                attn_piece, pcs_piece, accs_piece\n",
    "            )\n",
    "\n",
    "            if self.all_preds[i] == self.all_labels[i]:\n",
    "                for k, v in piece_agg.items():\n",
    "                    agg_correct[k] += v\n",
    "                count_correct += 1\n",
    "            else:\n",
    "                for k, v in piece_agg.items():\n",
    "                    agg_incorrect[k] += v\n",
    "                count_incorrect += 1\n",
    "\n",
    "        return agg_correct, agg_incorrect, count_correct, count_incorrect\n",
    "\n",
    "    def attention_entropy(self, attn_dict):\n",
    "        p = np.array(list(attn_dict.values()))\n",
    "        return -np.sum(p * np.log(p + 1e-9))\n",
    "\n",
    "    def get_mean_attn_dur_correlation_makam(self, makam):\n",
    "        corrs = []\n",
    "        makam_idx = self.get_makam_pieces(self.makam_vocab[makam])\n",
    "        for idx in makam_idx:\n",
    "            true_len_i = self.all_true_lengths[idx]\n",
    "            attn_piece_i = self.all_attn_weights[idx][:true_len_i]\n",
    "            durs_piece_i = self.all_durs[idx][:true_len_i]\n",
    "            corr_i = self.attention_duration_correlation(\n",
    "                attn_piece_i, durs_piece_i)\n",
    "            corrs.append(corr_i)\n",
    "\n",
    "        return np.array(corrs).mean()\n",
    "\n",
    "    def get_mean_attn_dur_correlation_all(self):\n",
    "        corrs = []\n",
    "        num_pieces = len(self.all_pcs)\n",
    "        for idx in range(num_pieces):\n",
    "            true_len_i = self.all_true_lengths[idx]\n",
    "            attn_piece_i = self.all_attn_weights[idx][:true_len_i]\n",
    "            durs_piece_i = self.all_durs[idx][:true_len_i]\n",
    "            corr_i = self.attention_duration_correlation(\n",
    "                attn_piece_i, durs_piece_i)\n",
    "            corrs.append(corr_i)\n",
    "        return np.array(corrs).mean()\n",
    "\n",
    "    def collect_attention_duration(self, target_makam=None):\n",
    "        \"\"\"\n",
    "        Returns a pandas DataFrame with columns:\n",
    "        ['duration', 'attention', 'makam']\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        num_pieces = len(self.all_pcs)\n",
    "        for i in range(num_pieces):\n",
    "            makam_id = self.all_labels[i]\n",
    "            makam_name = self.makam_vocab_inv[makam_id]\n",
    "\n",
    "            if target_makam is not None and makam_name != target_makam:\n",
    "                continue\n",
    "\n",
    "            T = len(self.all_durs[i])\n",
    "            for t in range(T):\n",
    "                if t == self.all_true_lengths[i]:\n",
    "                    break\n",
    "\n",
    "                dur_id = self.all_durs[i][t]\n",
    "                if dur_id == 0:\n",
    "                    continue  # PAD\n",
    "\n",
    "                dur_value = self.dur_vocab_inv[dur_id]\n",
    "                if dur_value == 0:\n",
    "                    continue  # ornaments\n",
    "\n",
    "                attn = self.all_attn_weights[i][t]\n",
    "\n",
    "                records.append({\n",
    "                    \"duration\": dur_value,\n",
    "                    \"attention\": attn,\n",
    "                    \"makam\": makam_name\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df_binned = self.rebin_relative_duration(df)\n",
    "        return df_binned\n",
    "\n",
    "    def rebin_relative_duration(self, df):\n",
    "        bins = [0.0, 0.0625, 0.1875, 0.375, 0.75, 1.01]\n",
    "        labels = [\n",
    "            \"Very short (≤1/16)\",\n",
    "            \"Short (≤3/16)\",\n",
    "            \"Medium (≤3/8)\",\n",
    "            \"Long (≤3/4)\",\n",
    "            \"Very long (>3/4)\"\n",
    "        ]\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"dur_bin\"] = pd.cut(\n",
    "            df[\"duration\"],\n",
    "            bins=bins,\n",
    "            labels=labels,\n",
    "            include_lowest=True\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def cliffs_delta(self, x, y):\n",
    "        nx, ny = len(x), len(y)\n",
    "        greater = sum(1 for xi in x for yi in y if xi > yi)\n",
    "        less = sum(1 for xi in x for yi in y if xi < yi)\n",
    "        return (greater - less) / (nx * ny)\n",
    "\n",
    "    def collect_attn_dur_bin_stats(self, target_makam=None):\n",
    "        df = self.collect_attention_duration(target_makam)\n",
    "        # print(df.head())\n",
    "        # pm.plot_attention_vs_duration_boxplot(df)\n",
    "\n",
    "        # kruskal\n",
    "        attention_by_bin = (\n",
    "            df\n",
    "            .groupby(\"dur_bin\", observed=False)[\"attention\"]\n",
    "            .apply(list)\n",
    "            .to_dict()\n",
    "        )\n",
    "        groups = [vals for vals in attention_by_bin.values() if len(vals) > 0]\n",
    "        H, p = kruskal(*groups)\n",
    "        print(f\"Kruskal-Wallis H = {H:.3f}, p-value = {p:.4e}\")\n",
    "\n",
    "        print(\"- \" * 30)\n",
    "\n",
    "        # post hoc Holm\n",
    "        # Prepare dataframe\n",
    "        data = []\n",
    "        for bin_name, vals in attention_by_bin.items():\n",
    "            for v in vals:\n",
    "                data.append({\"duration_bin\": bin_name, \"attention\": v})\n",
    "\n",
    "        df_ph = pd.DataFrame(data)\n",
    "        # print(df.head())\n",
    "\n",
    "        # Dunn post-hoc with Holm correction\n",
    "        dunn = sp.posthoc_dunn(\n",
    "            df_ph,\n",
    "            val_col=\"attention\",\n",
    "            group_col=\"duration_bin\",\n",
    "            p_adjust=\"holm\"\n",
    "        )\n",
    "\n",
    "        print(dunn)\n",
    "\n",
    "        print(\"- \" * 30)\n",
    "\n",
    "        # cliff's delta\n",
    "        delta = self.cliffs_delta(\n",
    "            attention_by_bin[\"Very short (≤1/16)\"],\n",
    "            attention_by_bin[\"Very long (>3/4)\"]\n",
    "        )\n",
    "\n",
    "        print(f\"Cliff's delta = {delta:.3f}\")\n",
    "\n",
    "        # chi^2\n",
    "        threshold = np.percentile(df[\"attention\"].to_numpy(), 95)\n",
    "\n",
    "        def prob_high_attention(attentions):\n",
    "            return np.mean(np.array(attentions) > threshold)\n",
    "\n",
    "        for bin_name, vals in attention_by_bin.items():\n",
    "            print(bin_name, prob_high_attention(vals))\n",
    "\n",
    "        # Example: short vs long\n",
    "        short = attention_by_bin[\"Very short (≤1/16)\"]\n",
    "        long = attention_by_bin[\"Very long (>3/4)\"]\n",
    "\n",
    "        table = [\n",
    "            [sum(np.array(short) > threshold), sum(\n",
    "                np.array(short) <= threshold)],\n",
    "            [sum(np.array(long) > threshold),  sum(np.array(long) <= threshold)]\n",
    "        ]\n",
    "\n",
    "        chi2, p, _, _ = chi2_contingency(table)\n",
    "        print(f\"Chi² = {chi2:.3f}, p = {p:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = AttentionInspector(device, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    all_f_names,\n",
    "    all_attn_weights,\n",
    "    all_true_lengths,\n",
    "    all_pcs,\n",
    "    all_accs,\n",
    "    all_meas,\n",
    "    all_durs\n",
    ") = ai.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    makam_vocab,\n",
    "    makam_vocab_inv,\n",
    "    pc_vocab,\n",
    "    pc_vocab_inv,\n",
    "    acc_vocab,\n",
    "    acc_vocab_inv,\n",
    "    dur_vocab,\n",
    "    dur_vocab_inv\n",
    ") = ai.export_vocabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d43b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "makam_names = makam_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f86bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(makam_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ai.evaluate_classification(all_labels, all_preds, makam_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1772ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=results[\"report\"])\n",
    "print(df.T)\n",
    "# df.T.to_excel(\"clf_report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7276d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"confusion_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23367cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_list = list(makam_names)\n",
    "for i, row in enumerate(results[\"confusion_matrix\"]):\n",
    "    row[i] = 0\n",
    "    print(mn_list[i], mn_list[np.argmax(row)], np.max(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_confusion_matrix(\n",
    "        self,\n",
    "        cm,\n",
    "        class_names,\n",
    "        title=\"Normalized Confusion Matrix\",\n",
    "        save_fig=False\n",
    "    ):\n",
    "        # Normalize row-wise\n",
    "        cm = cm.astype(float)\n",
    "        cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.set_theme(style=\"white\", font=\"Times\")\n",
    "\n",
    "        ax = sns.heatmap(\n",
    "            cm_norm,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            linewidths=0.5,\n",
    "            linecolor=\"gray\",\n",
    "            cbar_kws={\"label\": \"Proportion\"},\n",
    "            annot_kws={\"size\": 10}\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Predicted Makam\", fontsize=12)\n",
    "        ax.set_ylabel(\"True Makam\", fontsize=12)\n",
    "        ax.figure.axes[-1].yaxis.label.set_size(12)\n",
    "        ax.set_title(title, fontsize=12, pad=12)\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"figs/conf_matrix.pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention_over_time(\n",
    "            self,\n",
    "            attn,\n",
    "            title=\"Attention over Notes\",\n",
    "            save_fig=False\n",
    "    ):\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(attn, linewidth=2)\n",
    "        plt.xlabel(\"Note Index\", fontname=\"Times\", fontsize=12)\n",
    "        plt.ylabel(\"Attention Weight\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(title, fontname=\"Times\", fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"figs/attn_time.pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention_with_pitch(\n",
    "            self,\n",
    "            attn,\n",
    "            pcs,\n",
    "            pc_vocab_inv,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        pitches = [pc_vocab_inv[p] for p in pcs]\n",
    "\n",
    "        plt.figure(figsize=(24, 3))\n",
    "        plt.bar(range(len(attn)), attn)\n",
    "        plt.xticks(\n",
    "            range(len(pitches)),\n",
    "            pitches,\n",
    "            rotation=90,\n",
    "            fontsize=6,\n",
    "            fontname=\"Times\"\n",
    "        )\n",
    "        plt.xlabel(\"Note Index\", fontname=\"Times\", fontsize=12)\n",
    "        plt.ylabel(\"Attention Weight\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Attention Aligned with Pitch Classes\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"pitch_attn.pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention_with_measures(\n",
    "            self,\n",
    "            attn,\n",
    "            meas,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(attn, linewidth=2)\n",
    "\n",
    "        for i, m in enumerate(meas):\n",
    "            if m == 3:  # measure end {'PAD': 0, 'start': 1, 'middle': 2, 'end': 3}\n",
    "                plt.axvline(i, color=\"red\", alpha=0.2)\n",
    "\n",
    "        plt.xlabel(\"Note Index\", fontname=\"Times\", fontsize=12)\n",
    "        plt.ylabel(\"Attention Weight\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Attention with Measure Boundaries\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"meas_attn.pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention_with_combined_pitch(\n",
    "        self,\n",
    "        attn,\n",
    "        pcs,\n",
    "        accs,\n",
    "        ai: AttentionInspector,\n",
    "        save_fig=False\n",
    "    ):\n",
    "        pitch_labels = [\n",
    "            ai.combine_pitch_pc_acc(p, a)\n",
    "            for p, a in zip(pcs, accs)\n",
    "        ]\n",
    "\n",
    "        plt.figure(figsize=(24, 3))\n",
    "        plt.bar(range(len(attn)), attn)\n",
    "        plt.xticks(\n",
    "            range(len(pitch_labels)),\n",
    "            pitch_labels,\n",
    "            rotation=90,\n",
    "            fontsize=6,\n",
    "            fontname=\"Times\"\n",
    "        )\n",
    "        plt.xlabel(\"Note Index\", fontname=\"Times\", fontsize=12)\n",
    "        plt.ylabel(\"Attention Weight\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Attention Aligned with Combined Pitch Classes\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"comb_pitch_attn.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_agg_attention_by_pitch(\n",
    "            self,\n",
    "            agg,\n",
    "            ai: AttentionInspector,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        labels = []\n",
    "        values = []\n",
    "\n",
    "        for (pc, acc), val in sorted(agg.items(), key=lambda x: -x[1]):\n",
    "            pc_name = ai.pc_vocab_inv[pc]\n",
    "            acc_name = ai.acc_vocab_inv[acc]\n",
    "            labels.append(f\"{pc_name}{acc_name}\")\n",
    "            values.append(val)\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.bar(labels, values)\n",
    "        plt.xticks(rotation=45, fontname=\"Times\", fontsize=10)\n",
    "        plt.ylabel(\"Total Attention\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Aggregated Attention by Pitch + Accidental\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"agg_attn_by_pitch.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_agg_attention_by_pitch_duration(\n",
    "            self,\n",
    "            agg,\n",
    "            ai: AttentionInspector,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        labels = []\n",
    "        values = []\n",
    "\n",
    "        for (pc, acc), val in sorted(agg.items(), key=lambda x: -x[1]):\n",
    "            pc_name = ai.pc_vocab_inv[pc]\n",
    "            acc_name = ai.acc_vocab_inv[acc]\n",
    "            labels.append(f\"{pc_name}{acc_name}\")\n",
    "            values.append(val)\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.bar(labels, values)\n",
    "        plt.xticks(rotation=45, fontname=\"Times\", fontsize=10)\n",
    "        plt.ylabel(\"Total Attention\", fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Duration Weighted Aggregated Attention by Pitch + Accidental\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"agg_attn_by_pitch_dur.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_agg_attention_correct_vs_incorrect(\n",
    "            self,\n",
    "            makam,\n",
    "            ai: AttentionInspector,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        agg_c, agg_i, n_c, n_i = ai.aggregate_attention_correct_vs_incorrect(\n",
    "            ai.makam_vocab[makam])\n",
    "\n",
    "        agg_c = ai.normalize_attention(agg_c)\n",
    "        agg_i = ai.normalize_attention(agg_i)\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        for key in set(agg_c.keys()).union(agg_i.keys()):\n",
    "            pc, acc = key\n",
    "            label = f\"{ai.pc_vocab_inv[pc]}{ai.acc_vocab_inv[acc]}\"\n",
    "\n",
    "            rows.append({\n",
    "                \"Pitch\": label,\n",
    "                \"Attention\": agg_c.get(key, 0),\n",
    "                \"Type\": \"Correct\"\n",
    "            })\n",
    "            rows.append({\n",
    "                \"Pitch\": label,\n",
    "                \"Attention\": agg_i.get(key, 0),\n",
    "                \"Type\": \"Incorrect\"\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        plt.figure(figsize=(7, 3))\n",
    "        sns.barplot(data=df, x=\"Pitch\", y=\"Attention\", hue=\"Type\")\n",
    "        plt.xticks(rotation=45, fontname=\"Times\", fontsize=10)\n",
    "        plt.title(\n",
    "            f\"Attention by Pitch+Accidental: \"\n",
    "            f\"Correct vs Incorrect ({makam.capitalize()})\",\n",
    "            fontname=\"Times\", fontsize=12\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"attn_by_pitch_corr_incorr.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "        entropy_correct = ai.attention_entropy(agg_c)\n",
    "        entropy_incorrect = ai.attention_entropy(agg_i)\n",
    "\n",
    "        print(\"Entropy (Correct):\", entropy_correct)\n",
    "        print(\"Entropy (Incorrect):\", entropy_incorrect)\n",
    "\n",
    "    def plot_agg_attention_by_pitch_duration_makam(\n",
    "            self,\n",
    "            makam,\n",
    "            ai: AttentionInspector,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        makam_agg, piece_count = ai.aggregate_attention_for_makam(\n",
    "            ai.makam_vocab[makam])\n",
    "\n",
    "        makam_agg = ai.normalize_attention(makam_agg)\n",
    "        labels = []\n",
    "        values = []\n",
    "\n",
    "        for (pc, acc), val in sorted(\n",
    "            makam_agg.items(), key=lambda x: -x[1]\n",
    "        ):\n",
    "            labels.append(f\"{ai.pc_vocab_inv[pc]}{ai.acc_vocab_inv[acc]}\")\n",
    "            values.append(val)\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.bar(labels, values)\n",
    "        plt.xticks(rotation=45, fontname=\"Times\", fontsize=10)\n",
    "        plt.title(\n",
    "            f\"Aggregated Attention by Pitch + Accidental \"\n",
    "            f\"({makam.capitalize()})\",\n",
    "            fontname=\"Times\", fontsize=12\n",
    "        )\n",
    "        plt.ylabel(\"Normalized Attention\", fontname=\"Times\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"attn_by_pitch_corr_incorr.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention_vs_duration_boxplot(\n",
    "            self,\n",
    "            df,\n",
    "            title=None,\n",
    "            save_fig=False\n",
    "    ):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"dur_bin\",\n",
    "            y=\"attention\",\n",
    "            color=\"skyblue\",\n",
    "            showfliers=False\n",
    "        )\n",
    "\n",
    "        # sns.stripplot(\n",
    "        #     data=df,\n",
    "        #     x=\"dur_bin\",\n",
    "        #     y=\"attention\",\n",
    "        #     color=\"black\",\n",
    "        #     size=2,\n",
    "        #     alpha=0.25,\n",
    "        #     jitter=True\n",
    "        # )\n",
    "\n",
    "        plt.xlabel(\"Relative duration (binned)\",\n",
    "                   fontname=\"Times\", fontsize=12)\n",
    "        plt.ylabel(\"Attention weight\",\n",
    "                   fontname=\"Times\", fontsize=12)\n",
    "        plt.title(\"Attention vs Relative Duration (Binned)\",\n",
    "                  fontname=\"Times\", fontsize=12)\n",
    "        plt.xticks(fontname=\"Times\", fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_fig:\n",
    "            plt.savefig(\"attn_vs_dur_box.pdf\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PlotManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aefef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_confusion_matrix(\n",
    "    results[\"confusion_matrix\"],\n",
    "    makam_vocab.keys(),\n",
    "    title=\"Normalized Confusion Matrix for Makam Classification\",\n",
    "    # save_fig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 41  # index of the piece in the test set\n",
    "\n",
    "(\n",
    "    attn_piece,\n",
    "    pcs_piece,\n",
    "    accs_piece,\n",
    "    meas_piece,\n",
    "    durs_piece\n",
    ") = ai.get_single_piece_ai(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f03d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pitch_acc = ai.aggregate_attention_by_pitch_acc(\n",
    "    attn_piece, pcs_piece, accs_piece)\n",
    "agg_pitch_acc = ai.normalize_attention(agg_pitch_acc)\n",
    "\n",
    "pm.plot_agg_attention_by_pitch(agg_pitch_acc, ai)\n",
    "\n",
    "agg_duration = ai.aggregate_attention_pitch_acc_duration(\n",
    "    attn_piece, pcs_piece, accs_piece, durs_piece\n",
    ")\n",
    "agg_duration = ai.normalize_attention(agg_duration)\n",
    "\n",
    "pm.plot_agg_attention_by_pitch_duration(agg_duration, ai)\n",
    "\n",
    "makam = \"hicaz\"\n",
    "avg_corr = ai.get_mean_attn_dur_correlation_makam(makam)\n",
    "\n",
    "print(f\"Attention-Duration correlation ({makam}): {avg_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "makam = \"huseyni\"\n",
    "pm.plot_agg_attention_correct_vs_incorrect(makam, ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_agg_attention_by_pitch_duration_makam(\"huseyni\", ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_attention_over_time(\n",
    "    attn_piece,\n",
    "    title=\"Attention over Note Sequence\"\n",
    ")\n",
    "pm.plot_attention_with_pitch(\n",
    "    attn_piece,\n",
    "    pcs_piece,\n",
    "    pc_vocab_inv\n",
    ")\n",
    "pm.plot_attention_with_combined_pitch(\n",
    "    attn_piece,\n",
    "    pcs_piece,\n",
    "    accs_piece,\n",
    "    ai\n",
    ")\n",
    "pm.plot_attention_with_measures(attn_piece, meas_piece)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
